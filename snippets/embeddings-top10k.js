/**
 * Top 10K most common search terms with pre-computed embeddings
 *
 * PLACEHOLDER FILE - Generate with build-top10k.ts
 *
 * The actual file will contain:
 * - TOP_TERMS: Map of term -> Uint8Array (256-dim quantized embeddings)
 * - PCA_MATRIX: Float32Array[] for projecting 1024-dim embeddings to 256-dim
 * - TERM_TO_TITLE: Map of normalized search terms to canonical Wikipedia titles
 *
 * Size budget:
 * - 10K terms * 256 dims * 1 byte = 2.56 MB (too large!)
 *
 * Alternative: Store only 1K terms inline (~256KB), fetch rest from R2
 */

// Placeholder - will be generated by build script
// Top 1K terms for inline lookup (most common searches)
export const TOP_TERMS = new Map([
  // ['united states', new Uint8Array([...])],
  // ['world war ii', new Uint8Array([...])],
]);

// Term normalization map (search term -> canonical title)
export const TERM_TO_TITLE = new Map([
  ['usa', 'United States'],
  ['us', 'United States'],
  ['america', 'United States'],
  ['uk', 'United Kingdom'],
  ['ww2', 'World War II'],
  ['wwii', 'World War II'],
  ['ww1', 'World War I'],
  ['wwi', 'World War I'],
  // ... more aliases
]);

// PCA matrix for dimensionality reduction (256 x 1024)
// Each row projects a 1024-dim embedding to one dimension of the 256-dim space
export const PCA_MATRIX = null; // Will be Float32Array[] when generated

// Embedding dimension after PCA
export const REDUCED_DIM = 256;

// Original embedding dimension (bge-m3)
export const ORIGINAL_DIM = 1024;
